{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfLKSHbYmabw"
   },
   "source": [
    "**Name:** Muhammad Usman FAROOQ(55301764) and Rohith Saai PEMMASANI PRABAKARAN (54574572)\n",
    "\n",
    "**EID:** *mufarooq5* and *rpemmasan2*\n",
    "\n",
    "**Kaggle Team Name:** *Abelian Group*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBLBBpm6mabw"
   },
   "source": [
    "# CS4487 - Course Project: Aerial Cactus Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg5cBw2jmabw"
   },
   "source": [
    "## Motivation\n",
    "Increasing deforestation and wildfires have affected the flora and fauna in ecologically sensitive regions, leading to global warming and extinction of the rich ecosystem within these sensitive regions. The VIGIA project, led by a group of researchers in Mexico, aims to assist the goverment in its efforts to preserve the protected regions within the country.\n",
    "Traditional surveillance system have proven to be insufficient due to labour intensive nature of manual logging and the lack of innovations. Thus, new technology that allow automatic surveillence through the use of unmanned drones can be used to take images of the protected area landscape, with computer vision recognition system assisting in  distinguish the flora/fauna and logging the number of protected species, etc.\n",
    "In this project, our group has undertaken the task of building a computer vision recognition system that detects a certain species of columnar cactus in\n",
    "protected areas, from images compiled the VIGIA Project. [1]\n",
    "\n",
    "\n",
    "## Goal\n",
    "The goal of this project is to predict whether an image captured from an umanned drone contains the columnar cactus species. In Machine Learning, this problem is a typical case of a binary classification, where the developed model predicts if the image has the columnar cactus species or not, using the features present in the image.\n",
    "\n",
    "\n",
    "## Methodology\n",
    "*You need to train classifiers using the training data, and then predict on the test data. You are free to choose the feature extraction method and classifier algorithm.  You are free to use methods that were not introduced in class.  You should probably do cross-validation to select a good parameters.*\n",
    "\n",
    "\n",
    "## Evaluation on Kaggle\n",
    "\n",
    "You need to submit your test predictions to Kaggle for evaluation.  50% of the test data will be used to show your ranking on the live leaderboard.  After the assignment deadline, the remaining 50% will be used to calculate your final ranking. \n",
    "\n",
    "To submit to Kaggle you need to create an account, and use the competition invitation that will be posted on Canvas.\n",
    "\n",
    "**Note:** You can only submit 2 times per day to Kaggle!\n",
    "\n",
    "\n",
    "\n",
    "## Kaggle Notebooks\n",
    "\n",
    "You can use Kaggle notebooks to run your code. This ipynb has also been uploaded to the Kaggle competition site. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSrZDHZ3mabw"
   },
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DraYgSUkmabw"
   },
   "source": [
    "The class labels `\"1\"` for images containing cactus and `\"0\"` for others.\n",
    "\n",
    "To submit to Kaggle, you need to generate a Kaggle submission files, which is CSV file with the following format. `'id'` is the file name of the input image: \n",
    "\n",
    "<pre>\n",
    "Id,Prediction\n",
    "cactus_0181_18.jpg,1\n",
    "Sinplanta.4365.jpg,0\n",
    "...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCwaRaEnmabw"
   },
   "source": [
    "Here are two helpful functions for reading the data and writing the Kaggle submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzx7kqPrmabw"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "import seaborn as sn\n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "import csv\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import fnmatch\n",
    "import time\n",
    "import random\n",
    "random.seed(100)\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# OpenCV Image Library\n",
    "import cv2\n",
    "from PIL import Image as pilimg\n",
    "\n",
    "\n",
    "# Unzip/Create dataset folders\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Import PyTorch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision import *\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(linewidth=120)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bbnzxP8mabw",
    "outputId": "d90b0605-5d9c-4454-afe5-7c0e3c7b5f16"
   },
   "outputs": [],
   "source": [
    "#### IF USING KAGGLE TO RUN\n",
    "\n",
    "print(os.listdir('../input/cs4487-2020fall/'))\n",
    "def read_train_data():\n",
    "    cactus_imgs = glob(\"../input/cs4487-2020fall/training_set/training_set/cactus/*\")\n",
    "    cactus_labels = ones(len(cactus_imgs), dtype=int)\n",
    "    nocactus_imgs = glob(\"../input/cs4487-2020fall/training_set/training_set/no_cactus/*\")\n",
    "    nocactus_labels = zeros(len(nocactus_imgs), dtype=int)\n",
    "\n",
    "    train_X = cactus_imgs + nocactus_imgs\n",
    "    train_Y = hstack((cactus_labels, nocactus_labels))\n",
    "    return train_X, train_Y\n",
    "\n",
    "def read_test_data():\n",
    "    return glob(\"../input/cs4487-2020fall/validation_set/validation_set/*/*\")\n",
    "\n",
    "rootDir = \"../input/cs4487-2020fall/\"\n",
    "### CHECK AND PRINT STATEMENTS FOR READ DATA\n",
    "train_X, train_Y = read_train_data()\n",
    "print(train_X[0], train_Y[0])\n",
    "print(train_X[15000], train_Y[15000])\n",
    "print(len(train_X))\n",
    "\n",
    "test_X = read_test_data()\n",
    "print(test_X[0])\n",
    "print(len(test_X))\n",
    "print(os.path.basename(test_X[0]))\n",
    "\n",
    "trainingPath = 'training_set/'\n",
    "validationPath = 'validation_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING ON LOCAL MACHINE\n",
    "\n",
    "def read_train_data():\n",
    "    cactus_imgs = glob(\"training_set/cactus/*\")\n",
    "    cactus_labels = ones(len(cactus_imgs), dtype=int)\n",
    "    nocactus_imgs = glob(\"training_set/no_cactus/*\")\n",
    "    nocactus_labels = zeros(len(nocactus_imgs), dtype=int)\n",
    "\n",
    "    train_X = cactus_imgs + nocactus_imgs\n",
    "    train_Y = hstack((cactus_labels, nocactus_labels))\n",
    "    return train_X, train_Y\n",
    "\n",
    "def read_test_data():\n",
    "    return glob(\"validation_set/*/*\")\n",
    "\n",
    "### CHECK AND PRINT STATEMENTS FOR READ DATA\n",
    "train_X, train_Y = read_train_data()\n",
    "print(train_X[0], train_Y[0])\n",
    "print(train_X[15000], train_Y[15000])\n",
    "print(len(train_X))\n",
    "\n",
    "test_X = read_test_data()\n",
    "print(test_X[0])\n",
    "print(len(test_X))\n",
    "print(os.path.basename(test_X[0]))\n",
    "\n",
    "trainingPath = 'training_set/'\n",
    "validationPath = 'validation_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQ_MmzNzmabw"
   },
   "outputs": [],
   "source": [
    "def write_csv_kaggle_sub(fname, X, Y):\n",
    "    # fname = file name\n",
    "    # X is a list with image names\n",
    "    # Y is a list/array with class entries\n",
    "    \n",
    "    # header\n",
    "    tmp = [['Id', 'Prediction']]\n",
    "    \n",
    "    # add ID numbers for each Y\n",
    "    for x,y in zip(X, Y):\n",
    "        tmp2 = [x, y]\n",
    "        tmp.append(tmp2)\n",
    "        \n",
    "    # write CSV file\n",
    "    with open(fname, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chOk612-mabw"
   },
   "source": [
    "# Contents\n",
    "\n",
    "- Data Extraction, Preporcessing, and Understanding.\n",
    "- Utility Functions\n",
    "- Basic Model\n",
    "- Improved Vanilla CNN Model\n",
    "- Densenet191(Transfer Learning) Model - For testing purposes only use this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJw8aiQ6mabw"
   },
   "source": [
    "# DATA EXTRACTION, PREPROCESSING, AND UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKC8pt5lmabw"
   },
   "source": [
    "**Understanding the Problem**:\n",
    "- What data do we have?: Structured\n",
    "- What are we trying to predict? - Discrete/Niminal Prediction\n",
    "- What kind of task is it?: Classification and Supervised\n",
    "\n",
    "\n",
    "As we can see below we have two sets of images:\n",
    "\n",
    "- **Training and Validation set**: 17500 images - We divide training and validation by a split of *99.9/0.01* after trying multipl options like *80.0/20.0*. We will expand further in the split section.  \n",
    "- **Test set**: 4000 images\n",
    "\n",
    "**Data Preprocessing and Feature Engineering**\n",
    "Can an algorithm interpret our data?\n",
    "\n",
    "- Prepare data\n",
    "- Convert to PIL image for manipulation\n",
    "- Data Augmentation\n",
    "    - Resizing\n",
    "    - Random Flips\n",
    "    - Random Cropping\n",
    "    - Random Rotation\n",
    "    - Normalizing\n",
    "    - Color Jitters\n",
    "- Visualizing Data\n",
    "- Visualizing the Image Channels\n",
    "\n",
    "**Three Different Types of Models**\n",
    "\n",
    "- Lenet5CNN\n",
    "\n",
    "- Customized CNN\n",
    "\n",
    "- Transfer learning using Densenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv files to upload to dataset - change paths if not using on kaggle\n",
    "with open ('training_set.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range(len(train_Y)):\n",
    "        fname = os.path.basename(train_X[i])\n",
    "        writer.writerow([ fname,train_Y[i], train_X[i]])\n",
    "\n",
    "cactus_imgs = glob(\"../input/cs4487-2020fall/validation_set/validation_set/cactus/*\")\n",
    "cactus_labels = ones(len(cactus_imgs), dtype=int)\n",
    "nocactus_imgs = glob(\"../input/cs4487-2020fall/validation_set/validation_set/no_cactus/*\")\n",
    "nocactus_labels = zeros(len(nocactus_imgs), dtype=int)\n",
    "\n",
    "test_Y = hstack((cactus_labels, nocactus_labels))\n",
    "\n",
    "with open ('validation_set.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range(len(test_Y)):\n",
    "        fname = os.path.basename(test_X[i])\n",
    "        writer.writerow([fname,test_Y[i], test_X[i]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our own custom class for datasets\n",
    "class CactiDataset(Dataset):\n",
    "    def __init__(self, data, dataDirectory = './', transform=None):\n",
    "        super().__init__()\n",
    "        self.DF = data.values\n",
    "        self.data_dir = data.imagePath\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.DF)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name, label, imagePath = self.DF[index]\n",
    "        image = cv2.imread(imagePath)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aQXm8eZmabx"
   },
   "outputs": [],
   "source": [
    "## Turn CSVs into a Panda Dataframe\n",
    "\n",
    "#hasCactus == 1 indicates that the image is a cactus, hasCactus == 0 indicate that the image is not a cactus\n",
    "head = ['fileName','hasCactus', 'imagePath']\n",
    "trainDF = pd.read_csv(\"training_set.csv\", names = head, header = None)\n",
    "testDF = pd.read_csv(\"validation_set.csv\", names = head, header = None)\n",
    "\n",
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4zCOg-Cmabx"
   },
   "outputs": [],
   "source": [
    "print(\"Training Size: {}\".format(len(glob('../input/cs4487-2020fall/training_set/training_set/*/*.jpg'))))\n",
    "print(\"Validation Size: {}\".format(len(glob('../input/cs4487-2020fall/validation_set/validation_set/*/*.jpg'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNs7VFGmmabx"
   },
   "outputs": [],
   "source": [
    "# Counting the number of sample data for each class\n",
    "\n",
    "%matplotlib inline\n",
    "dataCount = trainDF.hasCactus.value_counts()\n",
    "\n",
    "plt.pie(dataCount, labels=['Has Cactus', 'No Cactus'], autopct='%1.1f%%', colors=['#32325d', '#6772e5'], textprops={'color': 'white'}, shadow=True) #e6ebf1, #87bbfd\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of RGB Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw26ks50mabx"
   },
   "source": [
    "The cactus images have three dimensions for their RGB values and all these three color dimensions are used in CNN model to extract features hence we will visualize and plot the color channels here to have a better view of the cactus images. \n",
    "\n",
    "To get a better idea about how these three color channels represent information and how they vary in their channel value distribution we also look at the normalized RGB color channels as three separate grayscale intensity images.\n",
    "\n",
    "Each little square in the  second visualization represents pixels with normalized values. Hence, larger values indicate a brighter pixel and smaller values indicate a dark pixel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tS_pDqImabx",
    "outputId": "01dd2900-ba9a-49b6-a645-e1a9d6136178"
   },
   "outputs": [],
   "source": [
    "from PIL import Image as pilimg\n",
    "\n",
    "# Select a random Image from the training set and then read that image in PIL format, then extract RGB channels individually and plot them. \n",
    "cactiDF = trainDF[trainDF.hasCactus==1].sample(n=1)\n",
    "sourceImage = pilimg.open(cactiDF.iloc[0, 2])\n",
    "sourceCactusImage = np.array(sourceImage)\n",
    "\n",
    "# plt.imshow(y, cmap = plt.get_cmap('gray'))\n",
    "fig = plt.figure(1, figsize=(20, 20))\n",
    "channels = ['Red', 'Green', 'Blue']\n",
    "for i in range(3):\n",
    "    ax = fig.add_subplot(1, 3, i+1)\n",
    "    # create empty image with same shape as that of the source image\n",
    "    extractedImage = np.zeros(sourceCactusImage.shape, dtype='uint8')\n",
    "    #assign the ith channel of the source to the empty image - hence i = 0 is Red , 1 is Green and 2 is Blue\n",
    "    extractedImage[:,:,i] = sourceCactusImage[:,:,i]\n",
    "    ax.imshow(extractedImage)\n",
    "    plt.ylabel('Height'.format(sourceCactusImage.shape[0]))\n",
    "    plt.xlabel('Width'.format(sourceCactusImage.shape[1]))\n",
    "    plt.title(\"Cactus channel: \" + channels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woPpKC0zmabx",
    "outputId": "c804aecb-1549-4f6a-bec1-f02936e97f20"
   },
   "outputs": [],
   "source": [
    "### TO HAVE A CLEAR UNDERSTANDING AND DISPLAY OF THE INTENSITY OF PIXEL VALUES WE RESIZE THE IMAGES TO 32 x 32 \n",
    "\n",
    "sourceCactusImage = np.array(sourceImage.resize((32, 32)))\n",
    "\n",
    "fig = plt.figure(1, figsize=(36, 36))\n",
    "for i in range(3):\n",
    "    ax = fig.add_subplot(3, 1, i+1)\n",
    "    # create empty image with same shape as that of the source image\n",
    "    extractedImage = np.zeros(sourceCactusImage.shape, dtype='uint8')\n",
    "    #assign the ith channel of the source to the empty image - hence i = 0 is Red , 1 is Green and 2 is Blue\n",
    "    extractedImage[:,:,i] = sourceCactusImage[:,:,i]\n",
    "    extractedGrayImage =  extractedImage / 255\n",
    "    extractedGrayImage = np.mean(extractedGrayImage, axis=2)\n",
    "    ax.imshow(extractedGrayImage, cmap=\"gray\")\n",
    "    width, height = extractedGrayImage.shape\n",
    "    threshold = extractedGrayImage.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(extractedGrayImage[x][y],2) if extractedGrayImage[x][y] !=0 else 0\n",
    "            ax.annotate(str(val), xy = (y, x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center', \n",
    "                    size=8,\n",
    "                    color='white' if extractedGrayImage[x][y] < threshold else 'black')\n",
    "    plt.ylabel('Height'.format(extractedGrayImage.shape[0]))\n",
    "    plt.xlabel('Width'.format(extractedGrayImage.shape[1]))\n",
    "    plt.title(\"Gray - Cactus channel: \" + channels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the above figures, we can make an assumption on how the CNN works. The CNN identifies the structure of the cactus through the different intensities of the red, green and blue channels, and its geometric position on the image. It is these values that the kernel uses to detect the presence of cactus in the image, and thus classifies it as present or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn3hYUFkmabx"
   },
   "source": [
    "# Utility Functions to Set up dataset and training in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXB2wuJJmabx"
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "trainingPath = 'training_set/'\n",
    "validationPath = 'validation_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for CUDA\n",
    "TRAIN_CUDA = torch.cuda.is_available()\n",
    "\n",
    "if not TRAIN_CUDA:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOuImL9gmabx"
   },
   "outputs": [],
   "source": [
    "# Our own custom class for datasets\n",
    "class CactiDataset(Dataset):\n",
    "    def __init__(self, data, dataDirectory = './', transform=None):\n",
    "        super().__init__()\n",
    "        self.DF = data.values\n",
    "        self.data_dir = data.imagePath\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.DF)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name, label, imagePath = self.DF[index]\n",
    "        image = cv2.imread(imagePath)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZJp2tTZmabx"
   },
   "outputs": [],
   "source": [
    "def splitTrainingData(trainData, batchSize=32, validationSize=0.2):\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(trainData)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(validationSize * num_train))\n",
    "    train_index, valid_index = indices[split:], indices[:split]\n",
    "\n",
    "    # Create Samplers\n",
    "    train_sampler = SubsetRandomSampler(train_index)\n",
    "    valid_sampler = SubsetRandomSampler(valid_index)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    trainLoader = DataLoader(trainData, batch_size=batchSize, sampler=train_sampler)\n",
    "    validLoader = DataLoader(trainData, batch_size=batchSize, sampler=valid_sampler)\n",
    "    return trainLoader, validLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCqTsy9Rmabx"
   },
   "outputs": [],
   "source": [
    "classes = [ 'No Cactus','Has Cactus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCU-rxeRmabx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotUnormalizedImages(train_loader):\n",
    "    # obtain one batch of training images\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    # display 10 unnormalized images\n",
    "    for index in np.arange(10):\n",
    "        ax = fig.add_subplot(2, 10/2, index+1, xticks=[], yticks=[])\n",
    "#         unnormalize\n",
    "        img = images[index] / 2 + 0.5   \n",
    "        # convert from Tensor image and display\n",
    "        plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "        ax.set_title(classes[labels[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_EO2Kufmabx"
   },
   "outputs": [],
   "source": [
    "def trainAndValidateModel(model, lossFunction, optimizer, epochs, trainLoader, validLoader, scheduler, isScheduler=False):\n",
    "    \n",
    "    '''\n",
    "    This utility function is used to train and validate models\n",
    "    It takes as input model, lossFunction, optimizer, \n",
    "    the number of epochs, trainLoader, validLoader, scheduler, and the boolean isScheduler which is by default False. Because we\n",
    "    aren't using scheduler in one of the models\n",
    "    \n",
    "    '''\n",
    "    print(\"Training Started...\")\n",
    "    \n",
    "    # We use this to track changes in the validation loss -  so that the best model can be used later for eval. \n",
    "    valid_loss_min = np.Inf \n",
    "\n",
    "    # History of training and validation losses for each epoch\n",
    "    trainingLoss = []\n",
    "    validationLoss = []\n",
    "    trainingAccuracy = []\n",
    "    validationAccuracy = []\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        epochStartTime = time.time()\n",
    "\n",
    "        # variables to keep track of per epoch loss and accuracy\n",
    "        trainLossEpoch = 0.0\n",
    "        validLossEpoch = 0.0\n",
    "        trainAccuracyEpoch = 0.0\n",
    "        validAccuracyEpoch = 0.0\n",
    "        accurateTrainPredictionsEpoch = 0.0\n",
    "        accurateValidPredictionsEpoch = 0.0\n",
    "\n",
    "        \"\"\"Train the model here - like any other CNN model we just train the model here\"\"\"\n",
    "        model.train()\n",
    "        for data, labels in trainLoader:\n",
    "             # move tensors to GPU if CUDA is available\n",
    "            if TRAIN_CUDA:\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "            # clear gradients of all the optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass - compute the predicted outputs by passing inputs to the model\n",
    "            prediction = model(data)\n",
    "            # Predicted class\n",
    "            predictedClass = prediction.argmax(dim=1)\n",
    "            # calculate the loss for the batch\n",
    "            loss = lossFunction(prediction, labels)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimization step - parameter update\n",
    "            optimizer.step()\n",
    "            # update training loss and accuracy\n",
    "            trainLossEpoch += loss.item()*data.size(0)\n",
    "            accurateTrainPredictionsEpoch += predictedClass.eq(labels).sum().item()\n",
    "            if isScheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "    #     trainingAccuracy += trainAccuracyEpoch\n",
    "\n",
    "        \"\"\"Now we validate the model and check for validation accuracy\"\"\"\n",
    "\n",
    "        model.eval()\n",
    "        for data, labels in validLoader:\n",
    "            if TRAIN_CUDA:\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "            prediction = model(data)\n",
    "            predictedClass = prediction.argmax(dim=1)\n",
    "            loss = lossFunction(prediction, labels)\n",
    "            # update average validation loss \n",
    "            validLossEpoch += loss.item()*data.size(0)\n",
    "            accurateValidPredictionsEpoch += predictedClass.eq(labels).sum().item()\n",
    "\n",
    "        # Calculate the accuracies for the epoch\n",
    "        validAccuracyEpoch = accurateValidPredictionsEpoch / len(validLoader.sampler)\n",
    "        trainAccuracyEpoch = accurateTrainPredictionsEpoch / len(trainLoader.sampler)\n",
    "        validationAccuracy.append(validAccuracyEpoch)\n",
    "        trainingAccuracy.append(trainAccuracyEpoch)\n",
    "\n",
    "        # calculate average losses\n",
    "        trainLossEpoch = trainLossEpoch /len(trainLoader.sampler)\n",
    "        validLossEpoch = validLossEpoch /len(validLoader.sampler)\n",
    "        trainingLoss.append(trainLossEpoch)\n",
    "        validationLoss.append(validLossEpoch)\n",
    "\n",
    "        hours, rem = divmod(time.time() - epochStartTime, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        epochTime = str(int(minutes)) + \":\" + str(int(seconds))\n",
    "\n",
    "\n",
    "        print('Epoch: {} - {} minutes \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n",
    "            epoch+1, epochTime, trainLossEpoch, validLossEpoch, validAccuracyEpoch*100))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if validLossEpoch <= valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f} Saving the model for after evaluations'.format(\n",
    "            valid_loss_min,\n",
    "            validLossEpoch))\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            valid_loss_min = validLossEpoch\n",
    "    \n",
    "    print(\"End Training...\")\n",
    "    return trainingLoss, validationLoss, trainingAccuracy, validationAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5TnNkrRmabx"
   },
   "outputs": [],
   "source": [
    "def plotLossCurves(trainingLoss, validationLoss):\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format = 'retina'\n",
    "    plt.plot(trainingLoss, label='Training loss', color=\"#32325d\")\n",
    "    plt.plot(validationLoss, label='Validation loss', color='#6772e5')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1N_XmBkmabx"
   },
   "outputs": [],
   "source": [
    "def plotAccuracyCurves(trainingAccuracy, validationAccuracy):\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format = 'retina'\n",
    "    plt.plot(trainingAccuracy, label='Training Accuracy', color=\"#32325d\")\n",
    "    plt.plot(validationAccuracy, label='Validation Accuracy', color='#6772e5')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax6uAN9Bmabx"
   },
   "outputs": [],
   "source": [
    "def getTrueAndPredictedLabels(model, loader):\n",
    "    targetLabels = []\n",
    "    modelPrediction = []\n",
    "\n",
    "    for images, label in loader:\n",
    "        if TRAIN_CUDA:\n",
    "            images, label = images.cuda(), label.cuda()\n",
    "        label = label.tolist()\n",
    "        targetLabels.append(label)\n",
    "\n",
    "        scores = model(images)\n",
    "        _, predictions = scores.max(1)\n",
    "        modelPrediction.append(predictions.tolist())\n",
    "\n",
    "    predictedLabels = [item for sublist in modelPrediction for item in sublist]\n",
    "    trueLabel = [item for sublist in targetLabels for item in sublist]\n",
    "    return trueLabel, predictedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2QJ2_XRmabx"
   },
   "outputs": [],
   "source": [
    "def confusionMatrix(trueLabel, predictedLabels, dataType=\"Test\"):\n",
    "    confusionMatrix = confusion_matrix(trueLabel, predictedLabels)\n",
    "    df_cm = pd.DataFrame(confusionMatrix, index = ['No cactus', 'Has cactus'],\n",
    "    columns = ['No cactus', 'Has cactus'])\n",
    "    df_cm.index.name = '{} Data - True Label'.format(dataType)\n",
    "    df_cm.columns.name = '{} Data - Predicted label'.format(dataType)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sn.set(font_scale=1.4)\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 20}, fmt = '2', cmap=\"PuBu\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAGL-Zv5mabx"
   },
   "outputs": [],
   "source": [
    "def PlotROCCurve(trueLabel, predictedLabels, dataType=\"Test\"):\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(trueLabel, predictedLabels)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='ROC curve (auc = {:.6f} %)'.format(auc), color=\"#32325d\")\n",
    "    plt.fill_between(x=fpr, y1=tpr,facecolor='#6772e5', alpha=0.5 )\n",
    "    plt.legend()\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return auc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJc2-RJLmabx"
   },
   "outputs": [],
   "source": [
    "def checkAccuracy(model, loader, dataType=\"Test\"):\n",
    "    trueLabel = []\n",
    "    predictedLabels = []\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        if TRAIN_CUDA:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "        scores = model(images)\n",
    "        predictions = scores.argmax(dim=1)\n",
    "        trueLabel.append(labels)\n",
    "        predictedLabels.append(predictions)\n",
    "        num_correct += (predictions).eq(labels).sum()\n",
    "        num_samples += predictions.size(0)\n",
    "    print(\"{} Accuracy:\".format(dataType))\n",
    "    print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F2Jd967mabx"
   },
   "source": [
    "# CNN - Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLSi8Wgemabx"
   },
   "source": [
    "The custom LeNet-5 network used in the original paper is configured as follows. It receives as input a 3-channel 32 × 32 image. Then, 6 convolution filters with size 5 × 5 are applied with stride one. Next, a max pooling operation is performed, using a kernel of 2 × 2 and stride 2. Then, a new set of 16 convolution kernels with size 5 × 5 and stride one is applied. Again a max pooling operation is performed, using a kernel of 2 × 2 and stride 2. Next, the features are flattened to a one dimension vector of size 400. Later, the three fully connected layers are applied with 120, 84, and 2 nodes respectively. Until this point, the output of the CNN is a vector of real numbers called logits. Therefore, a LogSoftMaxfunction is applied to convert the logits into a normalized probability distribution.\n",
    "\n",
    "The Loss Function used in the original paper is **Negative logarithmic likelihood loss function** and **Adam's Optimizer** with the hyperparameters as follows: **Learning rate 0.01, number of epochs 150, and batch size 2500**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWbny0Rkmabx"
   },
   "outputs": [],
   "source": [
    "class LeNet5CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5CNN, self).__init__()\n",
    "        self.firstConvolutionLayer = nn.Conv2d(3, 6, 5, stride=1)\n",
    "        # Max-pooling\n",
    "        self.firstMaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Convolution\n",
    "        self.secondConvolutionLayer = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        # Max-pooling\n",
    "        self.secondMaxPool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Fully connected layer\n",
    "        self.firstFullyConncetedLayer = nn.Linear(16*5*5, 120)   \n",
    "        self.secondFullyConncetedLayer = nn.Linear(120, 84)      \n",
    "        self.thirdFullyConncetedLayer = nn.Linear(84, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.firstMaxPool(F.relu(self.firstConvolutionLayer(x)))  \n",
    "        x = self.secondMaxPool(F.relu(self.secondConvolutionLayer(x)))\n",
    "        # Now Flatten to a one dimension vector of size 400 i.e 16*5*5 \n",
    "        x = x.view(-1, 16*5*5)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = F.relu(self.firstFullyConncetedLayer(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = F.relu(self.secondFullyConncetedLayer(x))\n",
    "        # FC-3\n",
    "        x = self.thirdFullyConncetedLayer(x)\n",
    "        logits = x\n",
    "        # After the third Fully connected layer Until the output of the CNN is a vector of real numbers called logits. Therefore, a LogSoftMax function is applied\n",
    "        # to convert the logits into a normalized probability distribution\n",
    "        logSoftMax = nn.LogSoftmax(dim=1)\n",
    "        probs = logSoftMax(logits) \n",
    "        return probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMXqvsDpmabx"
   },
   "outputs": [],
   "source": [
    "model = LeNet5CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbhK8oI7mabx"
   },
   "outputs": [],
   "source": [
    "# It's unclear from the original paper if the authors used a validation set or not hence this might results in different results\n",
    "# since we are using validation set with a 0.2 split. \n",
    "batchSize = 2500\n",
    "validationSize = 0.2\n",
    "\n",
    "trainingTransforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "testTransforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Create Test Data Set and Test Loader for the Model\n",
    "test_data = CactiDataset(data=testDF, dataDirectory=validationPath, transform=testTransforms)\n",
    "testLoader = DataLoader(test_data, batch_size=batchSize, shuffle=False)\n",
    "# Train Data and Train Loader for the model\n",
    "train_data = CactiDataset(data=trainDF, dataDirectory=trainingPath, transform=trainingTransforms)\n",
    "trainLoader, validLoader = splitTrainingData(train_data, batchSize, validationSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtdzM1Wvmabx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Visualize the unormalized images after transformations\n",
    "plotUnormalizedImages(trainLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tizG4KcDmabx"
   },
   "source": [
    "### Train and Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXD4gFZFmabx",
    "outputId": "69a050f8-13d5-4656-df9b-6ad78e3a012a"
   },
   "outputs": [],
   "source": [
    "model = LeNet5CNN()\n",
    "if TRAIN_CUDA:\n",
    "    model.cuda()\n",
    "lossFunction = nn.NLLLoss()\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.01)\n",
    "epochs = 150\n",
    "\n",
    "trainingLoss, validationLoss, trainingAccuracy, validationAccuracy = trainAndValidateModel(model, lossFunction, optimizer, epochs, trainLoader, validLoader, \"scheduler\", isScheduler=False)\n",
    "# Load Best parameters learned from training into our model to make predictions later\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-8zmBP-mabx",
    "outputId": "69fbc473-6828-40d7-b8c6-648b76c485c9"
   },
   "outputs": [],
   "source": [
    "checkAccuracy(model, testLoader, dataType=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGSEW_eDmabx",
    "outputId": "3d143649-afce-4271-95cb-e8be52e8342f"
   },
   "outputs": [],
   "source": [
    "checkAccuracy(model, validLoader, dataType=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cczgGSZKmabx",
    "outputId": "1032deed-0269-4d79-ea97-c02b4c9a44ff"
   },
   "outputs": [],
   "source": [
    "checkAccuracy(model, trainLoader, dataType=\"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdoBvx9Umabx"
   },
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAUQd7xXmabx",
    "outputId": "d9b0a61a-a668-41fc-b98f-c540650528d1"
   },
   "outputs": [],
   "source": [
    "plotAccuracyCurves(trainingAccuracy, validationAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8nqBdllmabx",
    "outputId": "b1e79e04-4269-4966-bf13-dcf309e6b812"
   },
   "outputs": [],
   "source": [
    "plotLossCurves(trainingLoss, validationLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvFP45S2mabx",
    "outputId": "c671e219-b443-47ac-a009-3a23e8a0c79e"
   },
   "outputs": [],
   "source": [
    "trueLabel, predictedLabels = getTrueAndPredictedLabels(model, trainLoader)\n",
    "confusionMatrix(trueLabel, predictedLabels,  dataType=\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K1XO-tCmaby",
    "outputId": "59a95ad0-6729-488d-ac93-82c6a8e11e1a"
   },
   "outputs": [],
   "source": [
    "trueLabel, predictedLabels = getTrueAndPredictedLabels(model, testLoader)\n",
    "confusionMatrix(trueLabel, predictedLabels,  dataType=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqli7QeEmaby",
    "outputId": "0cb0c4ce-185d-4de7-eeb1-1bdc23722816"
   },
   "outputs": [],
   "source": [
    "PlotROCCurve(trueLabel, predictedLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxPaBvYDmaby"
   },
   "source": [
    "# Improved Vanilla CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StcRORyhmaby"
   },
   "source": [
    "So after playing around for a while we applied an architecture such that we had a convolution layer which receives as input a 3-channel image and then, 6 convolution filters with size 3 × 3 are applied with stride one and padding one. Then, a new set of 32 filtered convolution kernels with size 3 × 3 , stride one and padding one is applied and then we have another set of same convolutional layers but with 64 filters followed by a last one which implements 128 filters.  On each layer of convolution, a max pooling operation is performed, using a kernel of 2 × 2 and stride two. Next, the features are flattened to a one dimension vector of size 400. Later, two fully connected layers are applied with 512 and 2 nodes respectively and then finally we also add a dropout layer with p=0.2.\n",
    "\n",
    "The Loss Function used in the improved version is **CrossEntropy** we tried others like Binary Cross Entropy too but the results didn't improve much and **Adam's Optimizer** with the hyperparameters as follows: **Learning rate 0.001, number of epochs 30, and batch size 64**. The results improved but they could have been improved further using a more detailed analysis of where this model was going wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hni2P9Tmaby"
   },
   "outputs": [],
   "source": [
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "         # define layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128*2*2, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "     \n",
    "    # define forward function\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 128 * 2 * 2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # don't need softmax here since we'll use cross-entropy as activation.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8XoeGhYmaby",
    "outputId": "bc010b23-ca91-4d26-9bc0-dc5bba2e9190"
   },
   "outputs": [],
   "source": [
    "model = ImprovedCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEvOFFZhmaby"
   },
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "validationSize = 0.2\n",
    "trainingTransforms = transforms.Compose([transforms.ToPILImage(), \n",
    "                                         transforms.RandomHorizontalFlip(), \n",
    "                                            transforms.RandomVerticalFlip(),\n",
    "                                            transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25),\n",
    "                                            transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(1, 1.1)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "testTransforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create Test Data Set and Test Loader for the Model\n",
    "test_data = CactiDataset(data=testDF, dataDirectory=validationPath, transform=testTransforms)\n",
    "testLoader = DataLoader(test_data, batch_size=batchSize, shuffle=False)\n",
    "# Train Data and Train Loader for the model\n",
    "train_data = CactiDataset(data=trainDF, dataDirectory=trainingPath, transform=trainingTransforms)\n",
    "trainLoader, validLoader = splitTrainingData(train_data, batchSize, validationSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eqa5qRrmaby",
    "outputId": "d16b6841-b056-4e4f-9b66-d59602840a8b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotUnormalizedImages(trainLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfihweqTmaby"
   },
   "source": [
    "### Train and Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbaQPUTRmaby"
   },
   "outputs": [],
   "source": [
    "model = ImprovedCNN()\n",
    "if TRAIN_CUDA:\n",
    "    model.cuda()\n",
    "learningRate = 0.001\n",
    "epochs = 30\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = learningRate/10, epochs=epochs, steps_per_epoch=len(trainLoader))\n",
    "\n",
    "trainingLoss, validationLoss, trainingAccuracy, validationAccuracy = trainAndValidateModel(model, lossFunction, optimizer, epochs, trainLoader, validLoader, scheduler, isScheduler=True)\n",
    "# Load Best parameters learned from training into our model to make predictions later\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpk4tKBOmaby"
   },
   "outputs": [],
   "source": [
    "checkAccuracy(model, testLoader, dataType=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxRoSrITmaby"
   },
   "outputs": [],
   "source": [
    "checkAccuracy(model, validLoader, dataType=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za5JuhVUmaby"
   },
   "outputs": [],
   "source": [
    "checkAccuracy(model, trainLoader, dataType=\"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6lI05fgmaby"
   },
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzLkk3Homaby"
   },
   "outputs": [],
   "source": [
    "plotAccuracyCurves(trainingAccuracy, validationAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzd61tXmmaby"
   },
   "outputs": [],
   "source": [
    "plotLossCurves(trainingLoss, validationLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKnbWBrdmaby"
   },
   "outputs": [],
   "source": [
    "trueLabel, predictedLabels = getTrueAndPredictedLabels(model, trainLoader)\n",
    "confusionMatrix(trueLabel, predictedLabels,  dataType=\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vx7d4Yapmaby"
   },
   "outputs": [],
   "source": [
    "trueLabel, predictedLabels = getTrueAndPredictedLabels(model, testLoader)\n",
    "confusionMatrix(trueLabel, predictedLabels,  dataType=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkAhj4Vqmaby"
   },
   "outputs": [],
   "source": [
    "PlotROCCurve(trueLabel, predictedLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hNFU-8ymaby"
   },
   "source": [
    "# Densenet / Transfer learning Model - Model used for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since multiple data ugmentation techniques along with different Learning rates and optimizers didn't yield us a ROC score of 1.00. We started with Pretrained Transfer Learning Models. We used noth ResNet and DenseNet because with our research these two models were the most famous and were expected to yield the best results. In the end fortunately Densenet gave us slightly better results then Resenet so we have included that here. \n",
    "\n",
    "**Dense Convolutional Network (DenseNet)**, connects each layer to every other layer in a feed-forward fashion. In comparison the traditional convolutional nets have N connections for N Layers - one between each layer and its subsequent layer. DenseNetsare known to solve the vanishing-gradient problem and have other improvememnts like substantial reduction of the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGD_GV7omaby"
   },
   "outputs": [],
   "source": [
    "class AdaptiveAvgPool(nn.Module): #for concatination of AdaptiveAvgPool2d and AdaptiveMaxPool2d\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.outputSize = size or 1\n",
    "        self.a = nn.AdaptiveAvgPool2d(self.outputSize)\n",
    "        self.m = nn.AdaptiveMaxPool2d(self.outputSize)\n",
    "\n",
    "    def forward(self, x): \n",
    "        return torch.cat([self.m(x), self.a(x)], 1)\n",
    "    \n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self, inputFeatures): # inputFeatures defines the number of channels in the last convolutional layer\n",
    "        super().__init__()\n",
    "        self.avg = AdaptiveAvgPool()\n",
    "        self.inputFeatures = inputFeatures\n",
    "        self.layer1 = nn.Sequential(nn.BatchNorm1d(2*inputFeatures), nn.Dropout(0.25),\n",
    "                                    nn.Linear(2*inputFeatures, 512), nn.ReLU(inplace=True))\n",
    "        self.layer2 = nn.Sequential(nn.BatchNorm1d(512), nn.Dropout(0.5),\n",
    "                                    nn.Linear(512, 2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.avg(x)\n",
    "        out = out.view(-1, self.inputFeatures*2)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "    \n",
    "class DensenetClf(nn.Module):\n",
    "    def __init__(self, freeze=True):\n",
    "        super().__init__()\n",
    "        base_model = models.densenet161(pretrained=True)\n",
    "        n_channels = base_model.classifier.in_features\n",
    "        self.body = nn.Sequential(base_model.features, nn.ReLU(inplace=True))\n",
    "        self.head = Sequence(n_channels)\n",
    "        \n",
    "        for x in self.body.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in self.body.modules():\n",
    "            if isinstance(x, nn.modules.batchnorm._BatchNorm):\n",
    "                x.bias.requires_grad = True\n",
    "                x.weight.requires_grad = True\n",
    "                x.reset_running_stats()\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return self.head(out)\n",
    "    \n",
    "    def predict(self, loader, device=torch.device('cpu')):\n",
    "        pred_Y = torch.tensor([])\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                image = batch[0].to(device)\n",
    "                current_pred = self(image)\n",
    "                pred_Y = torch.cat((pred_Y, current_pred.cpu().detach()))\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        pred_Y = softmax(y_pred)\n",
    "        return pred_Y[:, 1].numpy()\n",
    "    \n",
    "    def unfreeze(self):\n",
    "        for x in self.body.parameters():\n",
    "            x.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3S3xrFXmaby"
   },
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "validationSize = 0.01\n",
    "trainingTransforms = transforms.Compose([transforms.ToPILImage(), \n",
    "                                transforms.Resize((128,128)),\n",
    "                                transforms.RandomHorizontalFlip(), \n",
    "                                transforms.RandomVerticalFlip(),\n",
    "                                transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25),\n",
    "                                transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(1, 1.1)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "testTransforms = transforms.Compose([transforms.ToPILImage(), transforms.Resize((128,128)), transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "# Create Test Data Set and Test Loader for the Model\n",
    "test_data = CactiDataset(data=testDF, dataDirectory=validationPath, transform=testTransforms)\n",
    "testLoader = DataLoader(test_data, batch_size=batchSize, shuffle=False)\n",
    "# Train Data and Train Loader for the model\n",
    "train_data = CactiDataset(data=trainDF, dataDirectory=trainingPath, transform=trainingTransforms)\n",
    "trainLoader, validLoader = splitTrainingData(train_data, batchSize, validationSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learnRate = 3e-2\n",
    "epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "lossFunction = nn.CrossEntropyLoss(reduction='mean')\n",
    "tensorboard = False\n",
    "\n",
    "\n",
    "model = DensenetClf()\n",
    "model.to(device)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "print(f'\\nStart Training\\n')\n",
    "parameters = [{'params': model.body.parameters()},\n",
    "              {'params': model.head.parameters()}]\n",
    "optimizer = torch.optim.Adam(parameters, lr=learnRate)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=[learnRate/10, learnRate],\n",
    "                                                epochs=epochs, steps_per_epoch=len(trainLoader))\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "    train_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    start_time = datetime.now()\n",
    "    #training step\n",
    "    i = 0\n",
    "    model.train()\n",
    "    for image, label in tqdm(trainLoader, desc=\"Iteration\"):\n",
    "        image, label = image.to(device), label.to(device).squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(image)\n",
    "        loss = lossFunction(predicted_label, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        scheduler.step()\n",
    "    \n",
    "    #validate step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, label in validLoader:\n",
    "            image, label = image.to(device), label.to(device).squeeze()\n",
    "            predicted_label = model(image)\n",
    "            loss = lossFunction(predicted_label, label)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "\n",
    "    average_train_loss = train_loss/len(trainLoader)\n",
    "    average_valid_loss = validation_loss/len(validLoader)\n",
    "    delta = datetime.now()-start_time\n",
    "    print(f\"Epoch: {epoch}\\tTrain Loss: {average_train_loss:.6f}\\tVal Loss: {average_valid_loss:.6f}\\t Time:{delta}\")\n",
    "    torch.save(model.state_dict(), f'model.pt')\n",
    "\n",
    "\n",
    "print(\"End of Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkAccuracy(model, testLoader, dataType=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkAccuracy(model, trainLoader, dataType=\"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkAccuracy(model, validLoader, dataType=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAccuracyCurves(trainingAccuracy, validationAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLossCurves(trainingLoss, validationLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueLabel, predictedLabels = getTrueAndPredictedLabels(model, trainLoader)\n",
    "confusionMatrix(trueLabel, predictedLabels,  dataType=\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueLabel, predictedLabels = getTrueAndPredictedLabels(model, testLoader)\n",
    "confusionMatrix(trueLabel, predictedLabels,  dataType=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotROCCurve(trueLabel, predictedLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdxXO_DYmaby"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwDvyo5dmaby"
   },
   "outputs": [],
   "source": [
    "test_prediction = []\n",
    "\n",
    "model.eval()\n",
    "for batch in testLoader:\n",
    "    images = batch[0]  \n",
    "    images = images.to(device)  \n",
    "    scores = model(images)\n",
    "    _, predictions = scores.max(1)\n",
    "    test_prediction.append(predictions.tolist())\n",
    "\n",
    "test_pred = [item for sublist in test_prediction for item in sublist]\n",
    "test =  []\n",
    "for file in test_X:\n",
    "    fname = os.path.basename(file)\n",
    "    test.append(fname)\n",
    "\n",
    "write_csv_kaggle_sub('submission.csv', test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "In conclusion after using these three models where the original paper's model performed almost equal and sometimes worst than our improved model and the densenet model outperformed them all we were able to achieve higher accuracy scores with lower epochs each time from 150 epochs we brought it down to 10 epochs. However, we would have tried better ensembling methods and hyperparamter tuning methods like cyclic learning rates instead of relying on manual testing of hyperparameters like learning rate and batch sizes if we had better computational power and more time. The validation set size is important for hyperparameter tuning and to ensure that no overfitting occurs. The validation set needs to be a minimum of $1/(2n)^1/2$ of the dataset. Thus, in our case, the validation set needs to ~1%. We have also tried 20% and 10% and others using manual training, but again we would have tried better means to tune it and fine a better parameter rather than relying on the stanadard sizes used in various other models earlier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73Jn7-iGmaby"
   },
   "source": [
    "# Refrences \n",
    "\n",
    "- Efren López-Jiménez, Juan Irving Vasquez-Gomez, Miguel Angel Sanchez-Acevedo, Juan Carlos Herrera-Lozada, Abril Valeria Uriarte-Arcia, Columnar Cactus Recognition in Aerial Images using a Deep Learning Approach. Ecological Informatics. 2019.\n",
    "\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning (Adaptive computation and machine learning).\n",
    "\n",
    "- https://jivg.org/research-projects/vigia/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
